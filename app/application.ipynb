{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ed64d2",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548dbf15",
   "metadata": {},
   "source": [
    "Tách lọc nhãn trước khi embedding: Điều này giảm thiểu thời gian và tài nguyên cần thiết để xử lý dữ liệu không liên quan.\n",
    "Thêm cấu trúc rõ ràng hơn: Các bước như xử lý dữ liệu, tính embeddings, giảm chiều dữ liệu, và huấn luyện được phân tách rõ ràng.\n",
    "Thêm kiểm tra đường dẫn: Đảm bảo file tồn tại trước khi tải hoặc xử lý.\n",
    "Tối ưu hóa quá trình huấn luyện: Chuẩn hóa dữ liệu trước khi truyền vào mô hình.\n",
    "Đảm bảo mô-đun hóa: Các bước như tiền xử lý và trích xuất embeddings được đóng gói thành hàm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6f71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pyspark.sql import SparkSession\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ecf83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Paths for data and model storage\n",
    "BASE_PATH = 'C:/Users/vmcch/BIGDATA/sentiment_analysis_project/'\n",
    "W2V_EMBEDDINGS_PATH = \"C:/Users/vmcch/BIGDATA/sentiment_analysis_project/data/processed/\"  # Đường dẫn đến file .parquet chứa w2v embeddings\n",
    "MODEL_PATH = os.path.join(BASE_PATH, 'model/logistic_regression_w2v_model.joblib')\n",
    "SCALER_PATH = os.path.join(BASE_PATH, 'model/scaler_w2v.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7f5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_parquet(set_type, embedding_name=\"w2v\"):\n",
    "    \"\"\"Load dữ liệu từ file parquet\"\"\"\n",
    "    path = os.path.join(W2V_EMBEDDINGS_PATH, f\"{set_type}/{embedding_name}.parquet\")\n",
    "    df = spark.read.parquet(path)\n",
    "    return df\n",
    "\n",
    "# Load dữ liệu train và test với embedding w2v\n",
    "train_df = load_data_from_parquet(\"train\", \"w2v\")\n",
    "test_df = load_data_from_parquet(\"test\", \"w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2b81b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Chuyển đổi dữ liệu train và test\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m convert_to_numpy(test_df)\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36mconvert_to_numpy\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      4\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Chuyển cột embedding (Vector) thành numpy array\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtoArray() \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m pandas_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy()])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Chuyển cột label thành numpy array\u001b[39;00m\n\u001b[0;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m pandas_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Chuyển cột embedding (Vector) thành numpy array\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray() \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m pandas_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy()])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Chuyển cột label thành numpy array\u001b[39;00m\n\u001b[0;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m pandas_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "def convert_to_numpy(df):\n",
    "    \"\"\"Chuyển PySpark DataFrame sang numpy array để dùng với scikit-learn\"\"\"\n",
    "    # Chuyển PySpark DataFrame thành Pandas DataFrame\n",
    "    pandas_df = df.select(\"embedding\", \"label\").toPandas()\n",
    "    \n",
    "    # Chuyển cột embedding (Vector) thành numpy array\n",
    "    X = np.array([row[\"embedding\"].toArray() for row in pandas_df[[\"embedding\"]].to_numpy()])\n",
    "    \n",
    "    # Chuyển cột label thành numpy array\n",
    "    y = pandas_df[\"label\"].to_numpy()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Chuyển đổi dữ liệu train và test\n",
    "X_train, y_train = convert_to_numpy(train_df)\n",
    "X_test, y_test = convert_to_numpy(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb148da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaec27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
